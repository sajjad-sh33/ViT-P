train:
  # batch_size_per_gpu: 32
  batch_size_per_gpu: 28
  dataset_path: "Objects365"
  OFFICIAL_EPOCH_LENGTH: 21778
student:
  arch: vit_large
  patch_size: 14
  num_points: 100
  num_classes: 365
  # drop_path_rate: 0.4
  # ffn_layer: swiglufused
  # block_chunks: 4
optim:
  epochs: 5
  warmup_epochs: 1
  lr: 1e-2
crops:
  global_crops_size: [518, 518]
evaluation:
  eval_period_iterations: 21778 